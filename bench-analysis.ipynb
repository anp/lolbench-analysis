{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from os import path\n",
    "import json\n",
    "import numbers\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "glob('target-nightly-*')\n",
    "\n",
    "def getIndividualBenchmark(targetDir, benchName):\n",
    "    newDir = path.join(targetDir, 'criterion', benchName, 'new')\n",
    "    mainEstimates = path.join(newDir, 'estimates.json')\n",
    "    metricsEstimates = path.join(newDir, 'metrics-estimates.json')\n",
    "    \n",
    "    with open(metricsEstimates) as f:\n",
    "        metrics = json.loads(f.read())\n",
    "    with open(mainEstimates) as f:\n",
    "        metrics['nanoseconds'] = json.loads(f.read())\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def parseNightlyResult(targetDir):\n",
    "    benches = glob(path.join(targetDir, 'criterion/*'))\n",
    "    benchNames = [path.basename(b) for b in benches]\n",
    "    allMetrics = {benchName: getIndividualBenchmark(targetDir, benchName) for benchName in benchNames}\n",
    "    \n",
    "    return allMetrics\n",
    "\n",
    "rawResults = {tDir: parseNightlyResult(tDir) for tDir in glob('target-nightly-*')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform results to be a series keyed by benchmark name\n",
    "from datetime import datetime\n",
    "\n",
    "results = {datetime.strptime(d.replace('target-nightly-', ''), '%Y-%m-%d'): v for d, v in rawResults.items()}\n",
    "\n",
    "dates = set()\n",
    "benches = set()\n",
    "metricNames = set()\n",
    "for date, metrics in results.items():\n",
    "    dates.add(date)\n",
    "    benches.update(metrics.keys())\n",
    "    for _, metricName in metrics.items():\n",
    "        metricNames.update(k for k in metricName.keys())\n",
    "        \n",
    "# these metrics should always be *very* close to 0, so lets not investigate them right now\n",
    "dummyVariableMetrics = { 'align-faults', 'context-switches', 'cpu-migrations', 'emulation-faults', 'page-faults-major'}\n",
    "metricNames = metricNames.difference(dummyVariableMetrics)\n",
    "\n",
    "dates = sorted(list(dates))\n",
    "benches = sorted(list(benches))\n",
    "metricNames = set(sorted(list(metricNames)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descStatForGraphing(stat, daysNeeded=10):\n",
    "    forGraphing = {}\n",
    "    for bench in benches:\n",
    "        forBench = {}\n",
    "        maxDaysAvailable = 0\n",
    "        for metricName in metricNames:\n",
    "            estimatesAndErrors = []\n",
    "            daysAvailable = 0\n",
    "            for date in dates:\n",
    "                try:\n",
    "                    measure = results[date][bench][metricName]\n",
    "                    statistic = measure[stat]\n",
    "                    ci95 = statistic['confidence_interval']\n",
    "                    pointEstimate = statistic['point_estimate']\n",
    "                    error = (ci95['upper_bound'] - ci95['lower_bound']) / 2\n",
    "\n",
    "                    estimatesAndErrors.append((pointEstimate, error))\n",
    "                    daysAvailable += 1\n",
    "                except KeyError:\n",
    "                    estimatesAndErrors.append((float('nan'), float('nan')))\n",
    "\n",
    "            if daysAvailable > maxDaysAvailable:\n",
    "                maxDaysAvailable = daysAvailable\n",
    "            \n",
    "            estimates, errors= zip(*estimatesAndErrors)\n",
    "            forBench[metricName] = { \n",
    "                'estimate': np.array(estimates), \n",
    "                'errors': np.array(errors) \n",
    "            }\n",
    "        \n",
    "        if maxDaysAvailable >= daysNeeded:\n",
    "            forGraphing[bench] = forBench\n",
    "            \n",
    "    # with open('allresults-{}.json'.format(stat), 'w') as f:\n",
    "    #     f.write(json.dumps(forGraphing))\n",
    "    \n",
    "    return forGraphing\n",
    "\n",
    "meanForGraphing = descStatForGraphing('Mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "def ratios(forGraphing):\n",
    "    accessSuffix = '-access'\n",
    "    missSuffix = '-miss'\n",
    "    newForGraphing = copy(forGraphing)\n",
    "    for bench, newForBench in newForGraphing.items():\n",
    "        accessMetrics = [n for n in newForBench.keys() if n.endswith(accessSuffix)]\n",
    "            \n",
    "        for accessName in accessMetrics:\n",
    "            missName = accessName.replace(accessSuffix, missSuffix)\n",
    "            ratioName = accessName.replace(accessSuffix, '-ratio')\n",
    "\n",
    "            if accessName not in newForBench or missName not in newForBench:\n",
    "                continue\n",
    "            accesses = newForBench[accessName]['estimate']\n",
    "            misses = newForBench[missName]['estimate']\n",
    "\n",
    "            ratios = []\n",
    "            for access, miss in zip(accesses, misses):\n",
    "                if access == float('nan'):\n",
    "                    ratios.append(access)\n",
    "                elif access > 0:\n",
    "                    ratios.append((access - miss) / access)\n",
    "                else:\n",
    "                    ratios.append(1)\n",
    "\n",
    "            metricNames.add(ratioName)\n",
    "            newForBench[ratioName] = { 'estimate': np.array(ratios) }\n",
    "    return newForGraphing\n",
    "\n",
    "meanWithRatiosForGraphing = ratios(meanForGraphing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caches(forGraphing):\n",
    "    newForGraphing = copy(forGraphing)\n",
    "    for bench, newForBench in newForGraphing.items():\n",
    "        cacheRefs = newForBench['cache-references']['estimate']\n",
    "        cacheMisses = newForBench['cache-misses']['estimate']\n",
    "        cacheRatios = []\n",
    "        for ref, miss in zip(cacheRefs, cacheMisses):\n",
    "            if ref > 0:\n",
    "                cacheRatios.append((ref - miss) / ref)\n",
    "            else:\n",
    "                cacheRatios.append(ref)\n",
    "                \n",
    "        newForBench['cache-hit-ratio'] = { 'estimate': np.array(cacheRatios) }\n",
    "        metricNames.add('cache-hit-ratio')\n",
    "    return newForGraphing\n",
    "\n",
    "meanWithRatiosAndCachesForGraphing = caches(meanWithRatiosForGraphing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'bpu-read-access',\n",
       " u'bpu-read-miss',\n",
       " u'bpu-read-ratio',\n",
       " u'branch-instructions',\n",
       " u'branch-misses',\n",
       " u'bus-cycles',\n",
       " 'cache-hit-ratio',\n",
       " u'cache-misses',\n",
       " u'cache-references',\n",
       " u'cpu-clock',\n",
       " u'cpu-cycles',\n",
       " u'dtlb-read-access',\n",
       " u'dtlb-read-miss',\n",
       " u'dtlb-read-ratio',\n",
       " u'dtlb-write-access',\n",
       " u'dtlb-write-miss',\n",
       " u'dtlb-write-ratio',\n",
       " u'instructions',\n",
       " 'instructions-per-cycle',\n",
       " u'itlb-read-access',\n",
       " u'itlb-read-miss',\n",
       " u'itlb-read-ratio',\n",
       " u'l1d-read-access',\n",
       " u'l1d-read-miss',\n",
       " u'l1d-read-ratio',\n",
       " u'l1d-write-access',\n",
       " u'l1i-read-miss',\n",
       " u'll-read-access',\n",
       " u'll-read-miss',\n",
       " u'll-read-ratio',\n",
       " u'll-write-access',\n",
       " u'll-write-miss',\n",
       " u'll-write-ratio',\n",
       " 'nanoseconds',\n",
       " u'node-read-access',\n",
       " u'node-read-miss',\n",
       " u'node-read-ratio',\n",
       " u'node-write-access',\n",
       " u'node-write-miss',\n",
       " u'node-write-ratio',\n",
       " u'page-fault',\n",
       " u'page-fault-minor',\n",
       " u'ref-cpu-cycles',\n",
       " u'task-clock'}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ipc(forGraphing):\n",
    "    newForGraphing = copy(forGraphing)\n",
    "    for bench, newForBench in newForGraphing.items():\n",
    "        instructionCounts = newForBench['instructions']['estimate']\n",
    "        cycleCounts = newForBench['ref-cpu-cycles']['estimate']\n",
    "        ipcEstimates = []\n",
    "        for i, c in zip(instructionCounts, cycleCounts):\n",
    "            if c > 0:\n",
    "                ipcEstimates.append(i / c)\n",
    "            else:\n",
    "                ipcEstimates.append(float('nan'))\n",
    "\n",
    "        newForBench['instructions-per-cycle'] = { 'estimate': np.array(ipcEstimates) }\n",
    "        metricNames.add('instructions-per-cycle')\n",
    "    return newForGraphing\n",
    "        \n",
    "meanForGraphingWithOthers = ipc(meanWithRatiosAndCachesForGraphing)\n",
    "metricNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f84629e3e501482bbc2e0140e6c1034e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aW50ZXJhY3RpdmUoY2hpbGRyZW49KERyb3Bkb3duKGRlc2NyaXB0aW9uPXUnYmVuY2gnLCBvcHRpb25zPSgnYnJvdGxpXzFfMV8zOjpiZW5jaF9lMmVfZGVjb2RlX3E5XzVfMTAyNGsnLCAnYnLigKY=\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "%matplotlib osx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def drawGraph(bench):\n",
    "    print('starting graphing')\n",
    "    metrics = [\n",
    "        ('nanoseconds', { 'title': 'Wall Time (ns)' }), \n",
    "        ('instructions', { 'title': 'CPU Instructions' }),\n",
    "        ('ref-cpu-cycles', { 'title': 'CPU Cycles' }),\n",
    "        ('cache-hit-ratio', { 'title': 'Cache Hit Ratio' })\n",
    "    ]\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=len(metrics), ncols=1)\n",
    "    fig.suptitle(bench)\n",
    "    \n",
    "    for axis, (metricName, graphing) in zip(axes, metrics):\n",
    "        print('graphing {}'.format(metricName))\n",
    "        \n",
    "        means = meanForGraphingWithOthers[bench][metricName]['estimate']\n",
    "        meanErrors = meanForGraphingWithOthers[bench][metricName].get('errors', None)\n",
    "        \n",
    "        axis.set_title(graphing['title'])\n",
    "        \n",
    "        if 'ylabel' in graphing:\n",
    "            axis.set_ylabel(graphing['ylabel'])\n",
    "            \n",
    "        # figure out how much vertical space to devote, and also decide on\n",
    "        # an interval \n",
    "        highest = max(means)\n",
    "        lowest = min(means)\n",
    "        middle = np.median(means)\n",
    "        stdev = np.std(means)\n",
    "        midToLim = stdev * 3\n",
    "        \n",
    "        if midToLim < lowest:\n",
    "            bottom = middle - midToLim\n",
    "        else:\n",
    "            bottom = lowest - (0.15 * lowest)\n",
    "        \n",
    "        if midToLim > highest:\n",
    "            top = middle + midToLim\n",
    "        else:\n",
    "            top = highest + (0.15 * highest)\n",
    "        \n",
    "        axis.set_ylim(bottom=bottom, top=top)\n",
    "        \n",
    "        if meanErrors is not None:\n",
    "            axis.errorbar(x=dates, y=means, yerr=meanErrors, fmt='o')\n",
    "        else:\n",
    "            axis.plot(dates, means, 'o')\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(left=0.1, bottom=0.06, right=0.9, top=0.93, hspace=0.28)\n",
    "    fig.show()\n",
    "\n",
    "display(interactive(drawGraph, bench=sorted(list(meanForGraphingWithOthers.keys()))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
