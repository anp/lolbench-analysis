{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from os import path\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "glob('target-nightly-*')\n",
    "\n",
    "def getIndividualBenchmark(targetDir, benchName):\n",
    "    newDir = path.join(targetDir, 'criterion', benchName, 'new')\n",
    "    mainEstimates = path.join(newDir, 'estimates.json')\n",
    "    metricsEstimates = path.join(newDir, 'metrics-estimates.json')\n",
    "    \n",
    "    with open(metricsEstimates) as f:\n",
    "        metrics = json.loads(f.read())\n",
    "    with open(mainEstimates) as f:\n",
    "        metrics['nanoseconds'] = json.loads(f.read())\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def parseNightlyResult(targetDir):\n",
    "    benches = glob(path.join(targetDir, 'criterion/*'))\n",
    "    benchNames = [path.basename(b) for b in benches]\n",
    "    allMetrics = {benchName: getIndividualBenchmark(targetDir, benchName) for benchName in benchNames}\n",
    "    \n",
    "    return allMetrics\n",
    "\n",
    "rawResults = {tDir: parseNightlyResult(tDir) for tDir in glob('target-nightly-*')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform results to be a series keyed by benchmark name\n",
    "from datetime import datetime\n",
    "\n",
    "results = {datetime.strptime(d.replace('target-nightly-', ''), '%Y-%m-%d'): v for d, v in rawResults.items()}\n",
    "\n",
    "dates = set()\n",
    "benches = set()\n",
    "metricNames = set()\n",
    "for date, metrics in results.items():\n",
    "    dates.add(date)\n",
    "    benches.update(metrics.keys())\n",
    "    for _, metricName in metrics.items():\n",
    "        metricNames.update(k for k in metricName.keys())\n",
    "\n",
    "dates = sorted(list(dates))\n",
    "benches = sorted(list(benches))\n",
    "metricNames = sorted(list(metricNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers\n",
    "\n",
    "def descStatForGraphing(stat):\n",
    "    forGraphing = {b: {m: dict() for m in metricNames} for b in benches}\n",
    "    for bench in benches:\n",
    "        for metricName in metricNames:\n",
    "            estimatesAndErrors = []\n",
    "            for date in dates:\n",
    "                try:\n",
    "                    measure = results[date][bench][metricName]\n",
    "                    statistic = measure[stat]\n",
    "                    ci95 = statistic['confidence_interval']\n",
    "                    pointEstimate = statistic['point_estimate']\n",
    "                    error = (ci95['upper_bound'] - ci95['lower_bound']) / 2\n",
    "\n",
    "                    estimatesAndErrors.append((pointEstimate, error))\n",
    "                except KeyError:\n",
    "                    estimatesAndErrors.append((float('nan'), float('nan')))\n",
    "\n",
    "            estimates, errors= zip(*estimatesAndErrors)\n",
    "            forGraphing[bench][metricName] = { \n",
    "                'estimate': estimates, \n",
    "                'errors': errors \n",
    "            }\n",
    "    \n",
    "    with open('allresults-{}.json'.format(stat), 'w') as f:\n",
    "        f.write(json.dumps(forGraphing))\n",
    "    \n",
    "    return forGraphing\n",
    "\n",
    "meanForGraphing = descStatForGraphing('Mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8cb8eb46da1494f9c0927979f80c178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "aW50ZXJhY3RpdmUoY2hpbGRyZW49KERyb3Bkb3duKGRlc2NyaXB0aW9uPXUnYmVuY2gnLCBvcHRpb25zPSgnYnJvdGxpXzFfMV8zOjpiZW5jaF9lMmVfZGVjb2RlX3E1XzEwMjRrJywgJ2Jyb3TigKY=\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def drawGraph(bench):\n",
    "    plt.rcParams[\"figure.figsize\"] = (20,100)\n",
    "    plt.title(bench)\n",
    "    for i, metricName in enumerate(metricNames):\n",
    "        means = meanForGraphing[bench][metricName]['estimate'][7:30]\n",
    "        meanErrors = meanForGraphing[bench][metricName]['errors'][7:30]\n",
    "        \n",
    "        plt.subplot(len(metricNames) / 2, 2, i+1)\n",
    "        plt.title(metricName)\n",
    "        plt.errorbar(x=dates[7:30], y=means, yerr=meanErrors, fmt='o')\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "display(interactive(drawGraph, bench=benches))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
